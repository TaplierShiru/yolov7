{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344e5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from:\n",
    "# https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu\n",
    "\n",
    "import torch\n",
    "import onnxruntime\n",
    "import os\n",
    "from onnxruntime.quantization import (CalibrationDataReader, QuantFormat, \n",
    "                                      QuantType, quantize_static, quant_pre_process)\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "input_model_path = '/path/to/stored/onnx-model.onnx'\n",
    "# Preprocess model - model with input informaton. \n",
    "# This model will be create from input model for correct operation of quantization.\n",
    "preprocess_model_path = '/path/to/save/onnx-preprocess-model.onnx'\n",
    "output_model_path = '/path/to/save/model-quan.onnx'\n",
    "calibration_dataset_path = '/path/to/folders/with/frames'\n",
    "\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)\n",
    "\n",
    "\n",
    "def preprocess_image(image, new_shape):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image, ratio, dwdh = letterbox(image, new_shape=new_shape, auto=False)\n",
    "    image = image.transpose((2, 0, 1)).astype(np.float32, copy=False)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    image /= 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def _preprocess_images(images_folder: str, batch_size: int, height: int, width: int, size_limit=0):\n",
    "    \"\"\"\n",
    "    Loads a batch of images and preprocess them\n",
    "    parameter images_folder: path to folder storing images\n",
    "    parameter batch_size: batch size for the model\n",
    "    parameter height: image height in pixels\n",
    "    parameter width: image width in pixels\n",
    "    parameter size_limit: number of images to load. Default is 0 which means all images are picked.\n",
    "    return: list of matrices characterizing multiple images\n",
    "    \"\"\"\n",
    "    print('Prepare data...')\n",
    "    image_names = os.listdir(images_folder)\n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "    data_list = []\n",
    "    batch_data_list = []\n",
    "    for image_name in tqdm(batch_filenames):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        img = cv2.imread(image_filepath)\n",
    "        nchw_data = preprocess_image(img, new_shape=(height, width))\n",
    "        batch_data_list.append(nchw_data)\n",
    "        if len(batch_data_list) == batch_size:\n",
    "            data_list.append(\n",
    "                np.concatenate(batch_data_list, axis=0)\n",
    "            )\n",
    "            batch_data_list = []\n",
    "    print('Data is ready!')\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class DatasetDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str, size_limit=5_000):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(\n",
    "            model_path, None, \n",
    "            providers=['CPUExecutionProvider'],\n",
    "            #['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        )\n",
    "        (batch_size, _, height, width) = session.get_inputs()[0].shape\n",
    "        print(f'Input: {session.get_inputs()[0].shape}')\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.batched_data_list = _preprocess_images(\n",
    "            calibration_image_folder, batch_size, height, width, size_limit=size_limit\n",
    "        )\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.batched_data_list)\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = self.create_generator()\n",
    "        return next(self.enum_data, None)\n",
    "    \n",
    "    def create_generator(self, print_each=10, divide_by=5):\n",
    "        counter = 0\n",
    "        max_counter = len(self.batched_data_list)\n",
    "        while counter != max_counter:\n",
    "            yield {\n",
    "                self.input_name: self.batched_data_list[counter],\n",
    "            }\n",
    "            \n",
    "            counter += 1\n",
    "            if counter % print_each == 0:\n",
    "                loaded_percent = int((counter / max_counter) * 100)\n",
    "                not_loaded_percent_normed = (100-loaded_percent) // divide_by\n",
    "                loaded_percent_normed = loaded_percent // divide_by\n",
    "                print('+' * loaded_percent_normed, '-' * not_loaded_percent_normed)\n",
    "    \n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ee8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model_path):\n",
    "    session = onnxruntime.InferenceSession(\n",
    "        model_path, None, \n",
    "        providers=['CPUExecutionProvider'],\n",
    "        #['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    batch_size, channel, height, width = session.get_inputs()[0].shape\n",
    "\n",
    "    total = 0.0\n",
    "    runs = 10\n",
    "    input_data = np.zeros((batch_size, channel, height, width), np.float32)\n",
    "    # Warming up\n",
    "    _ = session.run([], {input_name: input_data})\n",
    "    for i in tqdm(range(runs)):\n",
    "        start = time.perf_counter()\n",
    "        _ = session.run([], {input_name: input_data})\n",
    "        end = (time.perf_counter() - start) * 1000\n",
    "        total += end\n",
    "        print(f\"{end:.2f}ms\")\n",
    "    total /= runs\n",
    "    print(f\"Avg: {total:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdac86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_pre_process(\n",
    "    input_model_path, preprocess_model_path,\n",
    "    skip_symbolic_shape=True, skip_optimization=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa657c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [16, 3, 640, 640]\n",
      "Prepare data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:30<00:00, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dr = DatasetDataReader(\n",
    "    calibration_dataset_path, input_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9274e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------------------\n",
      "+ ------------------\n",
      "+ ------------------\n",
      "++ -----------------\n",
      "+++ ----------------\n",
      "+++ ----------------\n",
      "++++ ---------------\n",
      "+++++ ---------------\n",
      "+++++ --------------\n",
      "++++++ -------------\n",
      "+++++++ -------------\n",
      "+++++++ ------------\n",
      "++++++++ -----------\n",
      "++++++++ -----------\n",
      "+++++++++ ----------\n",
      "++++++++++ ---------\n",
      "++++++++++ ---------\n",
      "+++++++++++ --------\n",
      "++++++++++++ --------\n",
      "++++++++++++ -------\n",
      "+++++++++++++ ------\n",
      "++++++++++++++ ------\n",
      "++++++++++++++ -----\n",
      "+++++++++++++++ ----\n",
      "++++++++++++++++ ----\n",
      "++++++++++++++++ ---\n",
      "+++++++++++++++++ --\n",
      "+++++++++++++++++ --\n",
      "++++++++++++++++++ -\n",
      "+++++++++++++++++++ \n",
      "+++++++++++++++++++ \n",
      "Calibrated and quantized model saved.\n"
     ]
    }
   ],
   "source": [
    "# Calibrate and quantize model\n",
    "# Turn off model optimization during quantization\n",
    "quantize_static(\n",
    "    preprocess_model_path,\n",
    "    output_model_path,\n",
    "    dr,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    # If you want all operations to be quantized - put None here\n",
    "    op_types_to_quantize=['Conv', 'Relu', 'Add', 'MatMul', 'Mul'],\n",
    "    per_channel=False,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QUInt8, # QInt8\n",
    "    optimize_model=False,\n",
    ")\n",
    "print(\"Calibrated and quantized model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2e0e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarking fp32 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:42<06:23, 42.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42598.93ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:28<05:49, 43.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46098.53ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:14<05:09, 44.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45601.54ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:56<04:22, 43.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42694.53ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:38<03:35, 43.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41696.89ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [04:21<02:52, 43.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43195.51ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [05:02<02:06, 42.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40207.47ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [05:42<01:23, 41.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40396.84ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [06:21<00:40, 40.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39291.61ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:04<00:00, 42.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42501.71ms\n",
      "Avg: 42428.36ms\n",
      "benchmarking int8 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [00:26<04:00, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26696.29ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:54<03:37, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28211.14ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:23<03:13, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28515.55ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:52<02:47, 27.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28501.50ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:21<02:21, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29205.83ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:50<01:53, 28.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28999.91ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:18<01:25, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27915.00ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:47<00:57, 28.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29199.97ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:16<00:28, 28.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29397.93ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:46<00:00, 28.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29098.38ms\n",
      "Avg: 28574.15ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"benchmarking fp32 model...\")\n",
    "benchmark(input_model_path)\n",
    "\n",
    "print(\"benchmarking int8 model...\")\n",
    "benchmark(output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e91d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d2a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ed0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c67ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994215c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import onnx\n",
    "from onnxruntime.quantization.qdq_loss_debug import (\n",
    "    collect_activations, compute_activation_error, compute_weight_error,\n",
    "    create_activation_matching, create_weight_matching,\n",
    "    modify_model_output_intermediate_tensors)\n",
    "\n",
    "import resnet50_data_reader\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--float_model\", required=True, help=\"Path to original floating point model\"\n",
    "    )\n",
    "    parser.add_argument(\"--qdq_model\", required=True, help=\"Path to qdq model\")\n",
    "    parser.add_argument(\n",
    "        \"--calibrate_dataset\", default=\"./test_images\", help=\"calibration data set\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def _generate_aug_model_path(model_path: str) -> str:\n",
    "    aug_model_path = (\n",
    "        model_path[: -len(\".onnx\")] if model_path.endswith(\".onnx\") else model_path\n",
    "    )\n",
    "    return aug_model_path + \".save_tensors.onnx\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Process input parameters and setup model input data reader\n",
    "    args = get_args()\n",
    "    float_model_path = args.float_model\n",
    "    qdq_model_path = args.qdq_model\n",
    "    calibration_dataset_path = args.calibrate_dataset\n",
    "\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "    print(\"Comparing weights of float model vs qdq model.....\")\n",
    "\n",
    "    matched_weights = create_weight_matching(float_model_path, qdq_model_path)\n",
    "    weights_error = compute_weight_error(matched_weights)\n",
    "    for weight_name, err in weights_error.items():\n",
    "        print(f\"Cross model error of '{weight_name}': {err}\\n\")\n",
    "\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "    print(\"Augmenting models to save intermediate activations......\")\n",
    "\n",
    "    aug_float_model = modify_model_output_intermediate_tensors(float_model_path)\n",
    "    aug_float_model_path = _generate_aug_model_path(float_model_path)\n",
    "    onnx.save(\n",
    "        aug_float_model,\n",
    "        aug_float_model_path,\n",
    "        save_as_external_data=False,\n",
    "    )\n",
    "    del aug_float_model\n",
    "\n",
    "    aug_qdq_model = modify_model_output_intermediate_tensors(qdq_model_path)\n",
    "    aug_qdq_model_path = _generate_aug_model_path(qdq_model_path)\n",
    "    onnx.save(\n",
    "        aug_qdq_model,\n",
    "        aug_qdq_model_path,\n",
    "        save_as_external_data=False,\n",
    "    )\n",
    "    del aug_qdq_model\n",
    "\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "    print(\"Running the augmented floating point model to collect activations......\")\n",
    "    input_data_reader = resnet50_data_reader.ResNet50DataReader(\n",
    "        calibration_dataset_path, float_model_path\n",
    "    )\n",
    "    float_activations = collect_activations(aug_float_model_path, input_data_reader)\n",
    "\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "    print(\"Running the augmented qdq model to collect activations......\")\n",
    "    input_data_reader.rewind()\n",
    "    qdq_activations = collect_activations(aug_qdq_model_path, input_data_reader)\n",
    "\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "    print(\"Comparing activations of float model vs qdq model......\")\n",
    "\n",
    "    act_matching = create_activation_matching(qdq_activations, float_activations)\n",
    "    act_error = compute_activation_error(act_matching)\n",
    "    for act_name, err in act_error.items():\n",
    "        print(f\"Cross model error of '{act_name}': {err['xmodel_err']} \\n\")\n",
    "        print(f\"QDQ error of '{act_name}': {err['qdq_err']} \\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ada4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e91ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'path/to/the/model.onnx'\n",
    "model_quant = 'path/to/the/model.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75683a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e7f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization.registry import QDQRegistry, QLinearOpsRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9facb2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conv': onnxruntime.quantization.operators.conv.QDQConv,\n",
       " 'Gemm': onnxruntime.quantization.operators.gemm.QDQGemm,\n",
       " 'Clip': onnxruntime.quantization.operators.activation.QDQRemovableActivation,\n",
       " 'Relu': onnxruntime.quantization.operators.activation.QDQRemovableActivation,\n",
       " 'Reshape': onnxruntime.quantization.operators.direct_q8.QDQDirect8BitOp,\n",
       " 'Transpose': onnxruntime.quantization.operators.direct_q8.QDQDirect8BitOp,\n",
       " 'Squeeze': onnxruntime.quantization.operators.direct_q8.QDQDirect8BitOp,\n",
       " 'Unsqueeze': onnxruntime.quantization.operators.direct_q8.QDQDirect8BitOp,\n",
       " 'Resize': onnxruntime.quantization.operators.resize.QDQResize,\n",
       " 'MaxPool': onnxruntime.quantization.operators.maxpool.QDQMaxPool,\n",
       " 'AveragePool': onnxruntime.quantization.operators.direct_q8.QDQDirect8BitOp,\n",
       " 'MatMul': onnxruntime.quantization.operators.matmul.QDQMatMul,\n",
       " 'Split': onnxruntime.quantization.operators.split.QDQSplit,\n",
       " 'Gather': onnxruntime.quantization.operators.gather.QDQGather,\n",
       " 'Softmax': onnxruntime.quantization.operators.softmax.QDQSoftmax,\n",
       " 'Where': onnxruntime.quantization.operators.where.QDQWhere}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QDQRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e2a2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ArgMax': onnxruntime.quantization.operators.argmax.QArgMax,\n",
       " 'Conv': onnxruntime.quantization.operators.conv.QLinearConv,\n",
       " 'Gemm': onnxruntime.quantization.operators.gemm.QLinearGemm,\n",
       " 'MatMul': onnxruntime.quantization.operators.matmul.QLinearMatMul,\n",
       " 'Add': onnxruntime.quantization.operators.binary_op.QLinearBinaryOp,\n",
       " 'Mul': onnxruntime.quantization.operators.binary_op.QLinearBinaryOp,\n",
       " 'Relu': onnxruntime.quantization.operators.activation.QLinearActivation,\n",
       " 'Clip': onnxruntime.quantization.operators.activation.QLinearActivation,\n",
       " 'LeakyRelu': onnxruntime.quantization.operators.activation.QLinearActivation,\n",
       " 'Sigmoid': onnxruntime.quantization.operators.activation.QLinearActivation,\n",
       " 'MaxPool': onnxruntime.quantization.operators.maxpool.QMaxPool,\n",
       " 'GlobalAveragePool': onnxruntime.quantization.operators.gavgpool.QGlobalAveragePool,\n",
       " 'Split': onnxruntime.quantization.operators.split.QSplit,\n",
       " 'Pad': onnxruntime.quantization.operators.pad.QPad,\n",
       " 'Reshape': onnxruntime.quantization.operators.direct_q8.Direct8BitOp,\n",
       " 'Squeeze': onnxruntime.quantization.operators.direct_q8.Direct8BitOp,\n",
       " 'Unsqueeze': onnxruntime.quantization.operators.direct_q8.Direct8BitOp,\n",
       " 'Resize': onnxruntime.quantization.operators.resize.QResize,\n",
       " 'AveragePool': onnxruntime.quantization.operators.pooling.QLinearPool,\n",
       " 'Concat': onnxruntime.quantization.operators.concat.QLinearConcat,\n",
       " 'Softmax': onnxruntime.quantization.operators.softmax.QLinearSoftmax,\n",
       " 'Where': onnxruntime.quantization.operators.where.QLinearWhere,\n",
       " 'Gather': onnxruntime.quantization.operators.gather.GatherQuant,\n",
       " 'Transpose': onnxruntime.quantization.operators.direct_q8.Direct8BitOp,\n",
       " 'EmbedLayerNormalization': onnxruntime.quantization.operators.embed_layernorm.EmbedLayerNormalizationQuant}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLinearOpsRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ac4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
